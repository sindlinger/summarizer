
# medical_summarizer/tokenizer.py 
from transformers import AutoTokenizer 
 
class MedicalTokenizer:     
    def __init__(self, model_name, model_path): 
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_path) 
    
        def tokenize(self, text): 
            return self.tokenizer.encode(text, return_tensors="pt") 
    
        def decode(self, token_ids): 
            return self.tokenizer.decode(token_ids[0], skip_special_tokens=True) 
